C:\Users\hjjh2\anaconda3\envs\pytorch\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
C:\Users\hjjh2\anaconda3\envs\pytorch\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
2023-03-14 14:15:01,997 -  [LOSS]] : 3.5549209117889404
2023-03-14 14:15:15,753 -  [LOSS]] : 0.9013152122497559
Traceback (most recent call last):
  File "main.py", line 143, in <module>
    main(args)
  File "main.py", line 139, in main
    train(train_loader, model, optimizer, criterion, args.print_freq, logger)
  File "main.py", line 52, in train
    loss.backward() #미분하면서 gradient 값이 생김
  File "C:\Users\hjjh2\anaconda3\envs\pytorch\lib\site-packages\torch\_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "C:\Users\hjjh2\anaconda3\envs\pytorch\lib\site-packages\torch\autograd\__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "C:\Users\hjjh2\anaconda3\envs\pytorch\lib\site-packages\wandb\wandb_torch.py", line 282, in <lambda>
    handle = var.register_hook(lambda grad: _callback(grad, log_track))
KeyboardInterrupt