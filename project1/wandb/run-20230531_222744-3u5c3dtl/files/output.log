Traceback (most recent call last):
  File "main.py", line 128, in <module>
    main(args)
  File "main.py", line 123, in main
    train(train_loader, model, optimizer, criterion, args.print_freq, logger)
  File "main.py", line 39, in train
    loss.backward() #미분하면서 gradient 값이 생김
  File "C:\Users\hjjh2\anaconda3\envs\pytorch\lib\site-packages\torch\_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "C:\Users\hjjh2\anaconda3\envs\pytorch\lib\site-packages\torch\autograd\__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "C:\Users\hjjh2\anaconda3\envs\pytorch\lib\site-packages\wandb\wandb_torch.py", line 282, in <lambda>
    handle = var.register_hook(lambda grad: _callback(grad, log_track))
KeyboardInterrupt